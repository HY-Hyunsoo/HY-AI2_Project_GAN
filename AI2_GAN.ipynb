{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch를 통한 Generative Adversarial Networks(GAN) 구현 및 이해<br/>**\n",
        "목표: 영어로 작성된 문서를 번역하고 구현 과정을 따라하며 GAN의 동작방식을 이해하고 코딩을 통해 실제 동작을 확인한다.<br/>\n",
        "[원문](https://www.run.ai/guides/deep-learning-for-computer-vision/pytorch-gan)"
      ],
      "metadata": {
        "id": "W4qJAZEZvvky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch GAN이란 무엇인가?<br/>**\n",
        "하나의 GAN(Generative Adversarial Networks)은 두 개의 뉴럴 네트워크를 이용한다. 하나의 생성자(Generator)와 하나의 판별자(Discriminator)가 그것이다. 이는 납득이 갈만한, 실제 데이터를 따라한 인위적인 데이터를 만들기 위함이다. 예를 들어, GAN 아키텍처는 동물이나 사람의 실사 같은 가짜 사진을 생성할 수 있다. <br/>\n",
        "PyTorch는 오픈소스 딥러닝 프레임워크를 이끌고 있다. PyTorch는 GAN 네트워크의 내장 도구들을 제공하지는 않지만, GAN 네트워크를 건설하기 위한 기초 요소들을 사용할 수 있게 해준다. 기초 요소에는 풀리 커넥티드 뉴럴 네트워크 레이어와 컨볼루셔널 레이어, 그리고 학습 기능 등이 있다."
      ],
      "metadata": {
        "id": "ms0q5iIJw3R-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**개요<br/>**\n",
        "하나의 GAN(Generative Adversarial Networks)는 두 개의 뉴럴 네트워크를 사용한다. 하나는 \"판별자\"로 알려져 있고, 다른 것은 \"생성자\"로 알려져 있으며, 서로를 적대한다. </br>\n"
      ],
      "metadata": {
        "id": "TgpzJ7S90X0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**판별자(Discriminator) <br/>**\n",
        "이것은 하나의 분류자로, 생성자로부터 제공받은 데이터를 분석하여 그것이 생성된 가짜 데이터인지 혹은 실제 데이터인지를 구별하려 한다. 학습은 실제 데이터 인스턴스를 긍정 예시로써, 생성자로부터 온 가짜 데이터 인스턴스를 부정 예시로써 이용하여 실행된다. <br/>\n",
        "손실 함수는 실제 데이터를 가짜로 오분류한 것, 또는 가짜 인스턴스를 실제의 것으로 오분류한 것을 처벌하는 용도이다. 판별자는 모든 학습 사이클에서, 손실 함수를 기반으로 한 역전파를 사용해 자신의 뉴럴 네트워크 가중치를 업데이트한다. 그리고 점점 가짜 데이터 인스턴스를 판별하는 것을 잘하게 된다."
      ],
      "metadata": {
        "id": "BByWj_2z1J1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**생성자(Generator)**<br/>\n",
        "생성자는 판별자로부터 온 피드백을 이용해 가짜 데이터를 생성하는 것을 학습한다. 이것의 목표는 자신의 결과물을 판별자가 실제 데이터 값으로 분류하는 일을 발생시키는 것이다.<br/>\n",
        "생성자를 학습시키기 위해서는 그것과 판별자를 단단히 통합시킬 필요가 있다. 학습은 임의의 입력 값을 취하며, 그것을 데이터 인스턴스로 변환하고, 판별자에게 그것을 제공해서 구분 결과를 받아온 뒤, 판별자로부터 올바른 판단에 의한 처벌 값인 생성자 손실을 계산한다.<br/>\n",
        "실험들은 생성자에게 초기에 주어진 임의의 노이즈 값이 어떠한 분산(쉬운 진행을 위해, 균일한 분산도 사용할 수 있다)도 가질 수 있다는 것을 보여준다.\n"
      ],
      "metadata": {
        "id": "ZSkh-l203kHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**생성자를 학습시키기 위해 판별자를 사용 <br/>**\n",
        "일반적인 뉴럴 네트워크를 학습시키기 위해 사용되는 과정은, 역전파 과정에서 손실 함수를 최소화하는 시도로써 가중치를 수정하는 것이다. 그러나 GAN에서는, 생성자가 판별자에게 값을 제공하고, 생성자 손실은 자신이 판별자를 속이는 데 실패한 것을 측정한다.<br/>\n",
        "이는 역전파 과정에 포함될 필요가 있다(이것은 판별자에서 생성자로의 환류(還流) 및 출력 때에 시작될 필요가 있다. 판별자가 학습하는 동안 판별자의 정지 상태를 유지하는 것이 중요하다).<br/>\n",
        "\n",
        "1. 생성자를 학습시키기 위해, 다음의 일반적인 과정을 이용:\n",
        "2. 초기의 임의의 노이즈 값을 얻고, 이를 생성자 출력을 생산하는 데 이용\n",
        "3. 임의의 노이즈 값의 출력물을 이용한 판별자의 판별 결과를 획득\n",
        "4. 판별자 손실을 계산\n",
        "5. 경사도(변화도)를 얻기 위해 판별자와 생성자 모두를 이용하여 역전파를 진행\n",
        "6. 오직 생성자의 가중치를 업데이트 하는 것에만 이 경사도들을 이용<br/>\n",
        "\n",
        "이는 모든 학습 사이클을 통해 생성자가 현재의 생성으로 판별자를 속이는, 조금 더 나은 출력 값을 만들어내도록 보장할 것이다."
      ],
      "metadata": {
        "id": "S1DeTeIA7C2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GAN 튜토리얼: PyTorch로 간단한 GAN 건설<br/>**\n",
        "이 짧은 튜토리얼은 [니콜라스 버타그놀리(Nicolas Bertagnolli)](https://towardsdatascience.com/build-a-super-simple-gan-in-pytorch-54ba349920e4)가 작성한 튜토리얼 및 코드를 기반으로 한다. 우리는 7개의 이진 값으로 짝수 숫자를 생성할 수 있는 간단한 생성자와 판별자를 만들 것이다. 이 예시의 실제 데이터는 “1,110,010”와 같은 유효한 숫자이며, 짝수이다."
      ],
      "metadata": {
        "id": "EwjJvUhaE7A5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**0. 라이브러리 임포팅 및 데이터 생성 함수 정의<br/>**"
      ],
      "metadata": {
        "id": "gkwlleIiGvQC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IUtfVWkGrj5_"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def create_binary_list_from_int(number: int) -> List[int]:\n",
        "    if number < 0 or type(number) is not int:\n",
        "        raise ValueError(\"Only Positive integers are allowed\")\n",
        "\n",
        "    return [int(x) for x in list(bin(number))[2:]]\n",
        "\n",
        "def generate_even_data(max_int: int, batch_size: int=16) -> tuple[List[int], List[List[int]]]:\n",
        "    # 최대 수를 나타내기 위해 요구되는 이진수 자리의 개수를 얻어온다\n",
        "    max_length = int(math.log(max_int, 2))\n",
        "\n",
        "    # 0-최대_자릿수 범위 내의 정수 값을 batch_size 숫자 만큼 샘플링 한다\n",
        "    sampled_integers = np.random.randint(0, int(max_int / 2), batch_size)\n",
        "\n",
        "    # 모든 수가 짝수이기 때문에 값이 모두 \"1\"인 라벨 리스트를 생성한다\n",
        "    labels = [1] * batch_size\n",
        "\n",
        "    # 학습을 위한 이진수 리스트를 생성한다\n",
        "    data = [create_binary_list_from_int(int(x * 2)) for x in sampled_integers]\n",
        "    data = [([0] * (max_length - len(x))) + x for x in data]\n",
        "\n",
        "    return labels, data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 생성자 건설<br/>**\n",
        "간결함을 유지하기 위해서, 우리는 이진 값을 7개의 자리에 맵핑하는 생성자 하나를 건설할 것이다(“0100111”과 같은 출력을 생성).\n",
        "이는 시그모이드 활성 함수를 사용하는 선형 레이어 하나만 이용하는 것으로도 충분하다."
      ],
      "metadata": {
        "id": "DwzlfminIElo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, input_length: int):\n",
        "    super(Generator, self).__init__()\n",
        "    self.dense_layer = nn.Linear(int(input_length), int(input_length))\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.activation(self.dense_layer(x))"
      ],
      "metadata": {
        "id": "llJL9wNIsFFM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 판별자 건설<br/>**\n",
        "판별자는 7개 자리의 이진 값을 입력으로 받고, 그것이 실제 데이터 분산(유효값, 짝수)에 위치되어 있는지 결정할 필요가 있다. 우리는 로지스틱 회귀와 시그모이드 활성 함수를 이용할 것이다."
      ],
      "metadata": {
        "id": "h8dS1ah6IPJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, input_length: int):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.dense = nn.Linear(int(input_length), 1);\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.activation(self.dense(x))"
      ],
      "metadata": {
        "id": "8_Tt6y-_I3a1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. 생성자와 판별자 학습<br/>**\n",
        "우리는 모델에게 다음 두 가지의 일괄적인 값을 건네주어 학습을 시작할 것이다:<br/>\n",
        "\n",
        "*   첫 번째는 임의의 노이즈 값이다.<br/>\n",
        "*   두 번째는 실제 값의 분포로부터 온 데이터를 포함한다.<br/>\n",
        "\n",
        "학습 함수는 다음과 같다:"
      ],
      "metadata": {
        "id": "1zLMrtDPJmRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(max_int: int = 128, batch_size: int = 16, training_steps: int = 500):\n",
        "  input_length = int(math.log(max_int, 2))\n",
        "\n",
        "  generator = Generator(input_length)\n",
        "  discriminator = Discriminator(input_length)\n",
        "\n",
        "  # 옵티마이저와 손실 함수 정의:\n",
        "  generator_optimizer = torch.optim.Adam(generator.parameters(), lr = 0.001)\n",
        "  discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr = 0.001)\n",
        "\n",
        "  loss = nn.BCELoss()\n",
        "\n",
        "  # 이제 각 학습 단계에서, 우리는 경사도를 0에 맞추고,\n",
        "  # 노이즈 데이터와 실제 데이터를 만든다:\n",
        "  for i in range(training_steps):\n",
        "    generator_optimizer.zero_grad()\n",
        "\n",
        "    noise = torch.randint(0, 2, size = (batch_size, input_length)).float()\n",
        "    generated_data = generator(noise)\n",
        "\n",
        "    true_labels, true_data = generate_even_data(max_int, batch_size = batch_size)\n",
        "    true_labels = torch.tensor(true_labels)\n",
        "    true_labels = true_labels.unsqueeze(dim = -1)\n",
        "    true_labels = true_labels.float()\n",
        "    true_data = torch.tensor(true_data).float()\n",
        "\n",
        "    # 이제 우리는 생성자를 학습시킨다. 이는 임의의 노이즈 값을 만들고, \"가짜\" 데이터를 생성하는 것,\n",
        "    # \"가짜\" 데이터의 라벨을 판별자가 예측하는 것, 그리고,\n",
        "    # 마치 실제 데이터인 것처럼 라벨을 이용해 판별자의 손실을 계산하는 것을 모두 포함한다.\n",
        "\n",
        "    # 판별자를 고정시킨채 생성자만을 위한 역전파가 실행된다.\n",
        "    generator_discriminator_out = discriminator(generated_data)\n",
        "    generator_loss = loss(generator_discriminator_out, true_labels)\n",
        "    generator_loss.backward()\n",
        "    generator_optimizer.step()\n",
        "\n",
        "    # 이제 우리는 판별자를 학습시키기 위해 가중치를 업데이트 한다.\n",
        "    # 이는 \"1\" 라벨을 갖는 한 무리의 실제 데이터를 전달하는 과정을 수반한다.\n",
        "    # 그리곤 생성자로부터 온 데이터를 떨어져 나온 가중치들 및 \"0\" 라벨과 함께 전달한다.\n",
        "    discriminator_optimizer.zero_grad()\n",
        "    true_discriminator_out = discriminator(true_data)\n",
        "    true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
        "\n",
        "    generator_discriminator_out = discriminator(generated_data.detach())\n",
        "\n",
        "    # 마침내, 우리는 두 과정을 통해 손실 함수 평균을 내고,\n",
        "    # 오직 판별자만 이용하여 역전파를 진행한다.\n",
        "    zero_labels = torch.zeros(batch_size)\n",
        "    zero_labels = zero_labels.unsqueeze(dim = -1)\n",
        "    generator_discriminator_loss = loss(generator_discriminator_out, zero_labels)\n",
        "    discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2\n",
        "    discriminator_loss.backward()\n",
        "    discriminator_optimizer.step()\n",
        "\n",
        "  return generator, discriminator"
      ],
      "metadata": {
        "id": "q3LalKOsKohR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 진행\n",
        "generator, discriminator = train()"
      ],
      "metadata": {
        "id": "pEEPYH_akGrJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "완성! 이 모델은 이제 납득할만한 유효한 7개 자리의 이진 숫자(짝수)를 생성할 수 있게 될 것이다.<br/>\n",
        "만약 이 \"장난감\" 도구를 넘어 더욱 전진하고자 한다면, 그리고 완전하고도 철저한, 컨볼루셔널/합성곱 레이어를 이용하여 가짜 이미지 생성, 실사와 같은 이미지를 포함한 것들을 생성할 수 있는 DCGAN을 건설하고자 한다면, 자세한 내용은 PyTorch 문서에 있는 [DCGAN 튜토리얼](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)을 참고하라."
      ],
      "metadata": {
        "id": "t_3kr14_K9ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch GAN Q&A<br/>**\n",
        "<br/>\n",
        "** GAN에서의 손실 함수는 무엇인가?<br/> **\n",
        "GAN 아키텍처는 확률 분표의 복제를 시도한다. 이들은 손실 함수를   GAN을 통해 생성된 데이터 분포가 GAN이 따라하려 시도하고 있는 실제 분포로부터 얼마나 멀리 떨어져 있는지 측정하기 위해 사용한다. <br/>\n",
        "GAN은 특히 다음 두 손실 함수를 갖는다: <br/>\n",
        "\n",
        "* 생성자 학습을 위한 것 <br/>\n",
        "* 판별자 학습을 위한 것 <br/>\n",
        "<br/>\n",
        "\n",
        "**Conditional GAN은 무엇인가?<br/>**\n",
        "Conditional GAN은 라벨링 된 데이터 셋의 학습과 각 생성된 인스턴스의 라벨 부여가 가능하다. 예를 들어, MNIST 데이터 셋으로 학습된 unconditional GAN은 임의의 숫자를 생성하는 데에 반해, conditional MNIST GAN은 GAN이 생성할 숫자를 특정 값으로 설정할 수 있다.<br/>\n",
        "<br/>\n",
        "**Progressive GAN은 무엇인가?<br/>**\n",
        "Progressive GAN에서는, 생성자의 가장 첫 레이어가 매우 저해상도인 이미지를 생산하고, 그 다음 레이어가 디테일을 추가한다. 이 기술은 Progressive GAN이 non-progressive GAN보다 더 빠르게 학습할 수 있게 해주며, 더 고해상도의 이미지를 생산할 수 있게 해준다.<br/>\n",
        "<br/>"
      ],
      "metadata": {
        "id": "gvVu6RyNN291"
      }
    }
  ]
}
